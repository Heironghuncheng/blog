---
title: "EFT阅读"
date: 2025-10-16T22:00:00+08:00
draft: false
featuredImg: ""
description : 'NIPS2024论文笔记——EFT'
tags: 
  - 论文笔记
  - MARL
author : BLESS
scrolltotop : true
toc : true
mathjax : true
comments: false
---

# EFT - 情景未来思维机制

这篇论文作者不多，仅两位，看起来像是学生与导师的组合，同样是国内作者。论文附带公开代码，整体感觉比 AgA 更易读。

论文地址：https://nips.cc/virtual/2024/poster/93443  
项目地址：https://github.com/DongsuLeeTech/EFTM

## 摘要

针对多智能体系统中智能体间的异构性问题，作者受动物认知过程启发，为每个智能体设计了一种**情景未来思维机制**（EFT）。首先，作者开发了一种**多角色策略**，用于捕捉多样化的行为特征。在多角色策略中，智能体的性格被定义为奖励组分的不同权重组合。具备 EFT 机制的智能体会收集目标智能体的观测-行动轨迹，利用训练好的多角色策略推断其性格特征。一旦完成推断，该智能体即可预测目标智能体的后续运动，对未来进行推演模拟，从而获得更高的奖励。

## 介绍

认知科学研究所表明，人类会运用反事实思维或未来情景模拟来优化决策。尽管反事实思维已在多智能体强化学习（MARL）中得到广泛应用，但情景未来思维的研究仍相对不足。

传统方法通常通过基于模型的强化学习进行未来轨迹预测，但这需要一个准确的状态转移模型。然而在 MARL 中，由于智能体间的复杂依赖关系，学习状态转移模型变得极其困难。因此，作者在部分可观测马尔可夫决策过程框架下，将奖励函数各部分的权重差异定义为角色特征（例如更偏好安全性或行驶速度），从而形成智能体间差异化的策略与行为模式。

实施 EFT 机制需要两个功能模块：多角色策略与角色推断模块。多角色策略覆盖不同角色的行为模式，支持在连续空间进行部分状态观测，并处理由离散与连续组成的混合动作空间。角色推断模块借鉴"逆向理性控制（IRC）"，即假设每个智能体都有一个并不完美的内部世界模型，寻找最能解释其行为的模型（概念本身也值得学习，参见 https://doi.org/10.48550/arXiv.2009.12576）。该模块通过最大化目标智能体观测—动作轨迹的对数似然来推断角色：在多角色策略的所有可能参数中，找到最可能产生这些动作的参数（即该角色）。两者结合使智能体具备 EFT 功能，从而在异构多智能体交互中预测他者行为并实现更主动的交互。

要使用 EFT 机制，智能体首先作为观察者收集目标智能体的观测—动作轨迹。随后利用角色推断模块得到目标智能体的角色特征。基于该特征并调用多角色策略，在固定自身动作为"无动作"的前提下预测其他智能体的行为，并模拟未来观测状态。通过这种心智模拟，智能体可估计"其他目标智能体已完成动作而自身尚未行动"时刻的观测，并据此选择最优动作。总体而言，EFT 使智能体能够在异构多智能体交互中实施前瞻性行为。

该机制确实具有前瞻性，但也存在几个潜在不足：其一，自身"无动作"的设定可能使未来观测的模拟存在偏差；其二，EFT 需要对可能出现的角色预先建模，即奖励由若干清晰的组成部分并可通过权重定义角色，这在奖励不明显或策略不明确（如稀疏奖励）场景中适用性受限。

## 相关工作

这篇论文的相关工作很有意思，值得仔细阅读。非 MARL 的相关工作比较多，对于认知科学和认知神经科学的研究，仅作简单翻译。

**情景未来思维**：认知神经科学旨在理解人类如何运用记忆进行决策。有研究指出，大脑区域神经激活在反事实推理（即模拟上一情景的替代后果）与未来思维（即模拟未来可能发生的情景）方面呈现出相似趋势，参见 https://onlinelibrary.wiley.com/doi/10.1111/jpr.12496。文献 https://www.pnas.org/doi/10.1073/pnas.1706589115 研究了未来思维与决策之间的关系，证实人类会执行面向未来的决策。在需要多智能体交互的场景中（例如社会决策），诸如策略制定等决策能力也具有重要意义。已经有很多研究致力于赋予智能体这种前瞻性的能力，比如有人做了面向智能汽车的多智能体联合的未来轨迹预测、基于时空相关性的短期交通状态预测，但这些多智能体系统预测都只预测了系统整体情况，没有对每个智能体给出预测；有人做了基于改进深度信念网络的横向与纵向驾驶行为预测，也仅预测了智能体的行为，没有显式地将预测结果用于决策上。智能体在搭载 EFT 的情况下，在当前观测和对其他智能体行为的预测的基础上能够预测未来可能的观测，从而做出前瞻性动作。

**基于模型的强化学习（MBRL）**：不管是已知环境模型还是使用最大似然估计学习，如果已有环境模型（指代状态转移方程和奖励函数），则可以预测未来轨迹。基于此，智能体可以用一些无模型强化学习用不了的宏观的全局的方法：Dreamer 利用预测轨迹的奖励来优化价值函数，而 MBPO 则将预测轨迹作为增强数据样本进行策略训练，也有工作将这两种算法扩展到了多智能体情况。但是 MBRL 过于理想化，首先必须有全局观测才可能对环境直接建模或者学习，且获取到的数据可能有噪声导致模型存在固有偏差，且智能体越多行为越复杂，这个环境模型就越不好构建。

**机器心智理论**：认知科学中存在心智理论（ToM），假设人类先天就能够以类推方式，假定其他人拥有与自己类似的心智，并根据这个假设来观察周围，作出合乎社会期望的反应与行动。或者工程点说，每个人都一个内部模型，人会对别人的内部模型进行估计来预测他的行为，然后做出决策。基于此有人提出了机器心智理论（Machine ToM），给智能体引入 ToM-net 来模拟人的 ToM 能力，ToM-net 包括角色嵌入、心理状态估计、行为预测三个模块，通过观测别的智能体学习他们的潜在意图，预测未来动作；在此基础上有人提出了动态系统 ToM，使用 MDP 实现 ToM；还有人实现了贝叶斯 ToM 框架增强人机团队协作；还有人实现了"推断的推断"框架，使用贝叶斯方法递归地建模多智能体推理过程，智能体不止预测其他智能体的行为，还会预测其他智能体如何看待自己，使用概率图模型捕捉嵌套推理在博弈任务中表现优秀；还有人提出了逆向理性控制，结合 ToM 用以从部分可观测的连续非线性动态中推断智能体的目标和意图，通过学习动态模型和逆向优化推测智能体的行为逻辑（目标函数等）。作者说上面这些套路只适用于智能体少、动作空间小、角色差异不大的场景，我没看出来为什么，但是直观上看计算量和网络复杂度应该是随智能体数量增加而迅速增加的。作者的工作和他们的相比，总的来看就是规定了智能体有哪些类型，提前把对应类型每个智能体的行为给训练出来，然后每个智能体在训练时看别的智能体只需要对号入座就行，另外他也把动作空间从连续的拓展到了混合型。

**错误共识效应（False Consensus Effect）**：是说心理学研究中发现，人会倾向于认为自身特质行为等具有普遍性，还是那篇 Machine ToM，他们发现 AI 也会有这个问题。所以作者为了比对 EFT 的重要性，设计了一个基于 FCE 的对照，对比智能体会认为其他智能体的角色与自身角色一致。

## 基于多角色策略的角色推断

### 多智能体决策的问题表述-MA-POMDP

定义 

$$
M = \langle E, S, {O_i}, {A_i}, T, {\Omega_i}, {R_i}, \gamma \rangle_{i \in E} 
$$

其中 \(E\) 是智能体的索引集合，连续状态 \(s_t \in S\) ，连续观测 \(o_{t,i} \in O_i\) ，混合动作 \(a_{i,t} = \{ a_{t,i}^c, a_{t,i}^d \} \in A_i\) ，其中 \(a_{t,i}^c\) 是连续动作，离散动作 \(a_{t,i}^d \in A_i^d = \{w: | w | \leq W, w \in \mathbb{Z}, W \in \mathbb{N}\}\) ，离散动作空间规模为 \(|A_i^d| = 2W + 1\) 。

令 \(A := A_1 \times A_2 \times \cdots \times A_N\) ，有 \(T: S \times A \to S\) 是状态转移概率， \(O_i: S \times A \to O_i\) 是观测概率， \(R_i: S \times A \to \mathbb{R}\) 是奖励函数， \(\gamma \in (0, 1)\) 是折扣因子。

时刻 \(t\) 所有智能体动作的无序集合记为 \(a_t = \langle a_{t,1}, \cdots, a_{t,i}, \cdots, a_{t,N} \rangle = \langle a_{t,i}, a_{t,-i} \rangle \in A\) ，其中下标 \(-i\) 表示集合 \(E\) 中除智能体 \(i\) 外所有智能体的索引。因此， \(a_{t,-i} = \langle a_{t,1}, \cdots, a_{t,i-1}, a_{t,i+1} \cdots, a_{t,N} \rangle\) 表示时刻 \(t\) 除 \(a_{t,i}\) 外所有智能体的动作集合。状态转移概率记为 \(T(s_{t+1}|s_t, a_t)\) 。注意，状态转移基于所有智能体的动作组合 \(a_t\) ，而非单个智能体的动作 \(a_{t,i}\) 。

接下来， \(c_i = \{c_{i,1}, c_{i,2}, \cdots, c_{i,K}\} \in C \in \mathbb{R}^K\) 表示智能体 \(i\) 的 \(K\) 维角色向量。角色 \(c_i\) 可以参数化智能体 \(i\) 的奖励函数，即 \(R_{t,i} = R_i(s_t, a_{t,i}, s_{t+1}; c_i)\) 。直观理解：奖励函数取决于当前状态、下一状态、动作以及角色。

智能体的目标是学习最优策略，在给定观测和角色的情况下返回最优动作 \(a^*_{t,i} \sim \pi^*(\cdot|o_{t,i}; c_i)\) 。具体而言，智能体的目标是通过构建最佳策略 \(\pi\) 来最大化期望折扣累积奖励：

$$
J(\pi) = E_\pi \left[ \sum_t \gamma^t R_i(s_t, a_{t,i}, s_{t+1}; c_i) \right]
$$

这定义了状态-动作价值函数：

$$
Q^\pi(s, a; c_i) = E_\pi \left[ \sum_t \gamma^t R_i(s_t, a_{t,i}, s_{t+1}; c_i) \bigg| s_0 = s, a_0 = a \right]
$$

### 训练多角色策略

多角色策略包括连续空间的输入（如观测 \(o_{t,i}\) 和角色 \(c_i\) ）和混合空间的输出（如动作 \(a_{t,i}\) ）。为了构建在连续空间上泛化的策略，作者采用了 Actor-Critic 架构。该架构近似策略 \(\pi_\phi(\cdot|o_{t,i}; c_i)\) 和 Q 函数 \(Q_\theta(o_{t,i}, a_{t,i}; c_i)\) ，其中 \(\phi\) 表示 Actor 网络的参数， \(\theta\) 表示 Critic 网络的参数。

用于训练 Actor 和 Critic 网络的损失函数分别为：
- \(L(\phi) = -Q_\theta(o_{t,i}, \pi_\phi(\cdot|o_{t,i}; c_i))\)
- \(L(\theta) = |y_t - Q_\theta(o_{t,i}, \pi_\phi(\cdot|o_{t,i}; c_i))|^2\)

其中， \(y_t = R_{t,i} + Q_{\theta'}(o_{t+1,i}, \pi_{\phi'}(\cdot|o_{t+1,i}; c_i))\) 表示时序差分（TD）目标， \(\theta'\) 和 \(\phi'\) 表示目标网络。

接下来，作者提出了一个后处理器 \(g(\cdot)\) 来处理混合动作空间。设原型动作 \(\bar{a}^d_{t,i}\) 为 Actor 网络的输出。后处理器 \(g(\cdot)\) 通过将连续的原型动作 \(\bar{a}^d_{t,i}\) 离散化为离散的后动作 \(a^d_{t,i}\) 来执行量化过程，即：

$$
a^d_{t,i} = g(\bar{a}^d_{t,i}, W) = \min\left(\left\lfloor \frac{2W + 1}{2W} \left( \bar{a}^d_{t,i} + \frac{W}{2W + 1} \right) \right\rfloor, W\right)
$$

其中 \(\lfloor \cdot \rfloor\) 表示向下取整函数。

后处理函数构建过程推导如下，先证明区间映射规则：

$$
a^d_t = w, \text{ 如果 } w - \frac{W + w}{2W + 1} < \bar{a}^d_t \leq w + \frac{W - w}{2W + 1}
$$

为了让每个连续值都均匀地映射到某个离散值，区间不重叠没空洞，边界无特殊处理，就需要每个区间的长度都为 \(\frac{2W}{2W + 1}\) ，区间可以写成以 \(w\) 为中心的左右偏移，即：

$$
w - L(w) \leq \bar{a}^d_t < w + R(w)
$$

再令连续两个区间边界对齐有：

$$
w + R(w) = (w + 1) - L(w + 1)
$$

左右边界需对齐有

$$
\begin{aligned}
L(- W) &= - W \\
R(W) &= W
\end{aligned}
$$

设左右偏移随 \(w\) 线性变化，解的表达式为：

$$
\begin{aligned}
L(w) &= w - \frac{W + w}{2W + 1} \\
R(w) &= w + \frac{W - w}{2W + 1}
\end{aligned}
$$

该条件可以写成 \(a^d_t = w\) 的范围：

$$
\frac{2W + 1}{2W} \left( \bar{a}^d_t - \frac{W}{2W + 1} \right) \leq w < \frac{2W + 1}{2W} \left( \bar{a}^d_t + \frac{W}{2W + 1} \right)
$$

它可以重新表述为：

$$
w = \min\left( \left\lfloor \frac{2W + 1}{2W} \left( \bar{a}^d_t + \frac{W}{2W + 1} \right) \right\rfloor, W \right)
$$

其中 \(\min(\cdot, W)\) 防止 \(w\) 超出动作空间 \([-W, W]\) 的范围。这里，在不等式的右侧使用了向下取整函数。在该不等式左侧也可以使用向上取整函数作为替代，配合最大值函数 \(\max(\cdot, -W)\) 。

后处理器函数 \(a^d_t = g(\bar{a}^d_t, W)\) 最终表述如下：

$$
g(\bar{a}^d_t, W) = \min\left( \left\lfloor \frac{2W + 1}{2W} \left( \bar{a}^d_t + \frac{W}{2W + 1} \right) \right\rfloor, W \right)
$$

训练时，每个 episode 开始随机选择一个角色 \(c_i\) 应用于该 episode，Actor 仅输出连续值 \(\{ a_{t,i}^c, \bar{a}^d_{t,i} \}\) ，随后通过后处理器 \(g(\cdot)\) 得到离散动作 \(a^d_{t,i}\) 。

### 推理目标智能体的角色

完成多角色策略的训练后，需要推断目标智能体 \(j \in E\) 的角色 \(c_j\) 。智能体首先收集目标智能体的观测-动作轨迹用于角色推断。随后，它利用多角色策略来识别最能解释收集数据的角色 \(c_j\) ，可以通过最大化观测-动作轨迹的对数似然 \(\ln P(o_{1:T,j}, a_{1:T,j}|c_j)\) 来估计。这可以表述如下：

$$
\hat{c}_j = \arg \max_c \ln P(o_{1:T,j}, a_{1:T,j}|c) = \arg \max_c \sum_{t=1}^T [\ln \pi(a^c_{t,j}|o_{t,j}; c) + \ln \pi(a^d_{t,j}|o_{t,j}; c)]
$$

其证明过程如下：

为使上式可计算，我们从对数证据开始，构造一个可优化的下界并逐步分解到策略项。令 \(q(s_{1:T}) := P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j} \mid c)\) （后验状态分布），有：

$$
\hat{c}_j
= \arg\max_c \ln P(o_{1:T,j}, a_{1:T,j} \mid c)
= \arg\max_c \ln \int  P(o_{1:T,j}, a_{1:T,j} \mid c) \, ds_{1:T}
$$

等价地，分子分母同乘 \(P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)\) 可写为：

$$
= \arg\max_c \ln \int P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c) \times \frac {P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c)} {P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)} \, ds_{1:T}
$$

将 \(P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)\) 视为某随机变量的概率密度函数， \(\frac {P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c)} {P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)}\) 视为作用于该随机变量上的某函数，对凹函数 \(\ln(\cdot)\) 使用 Jensen 不等式（描述凹凸函数，期望的函数和函数的期望关系的不等式） \(f(\mathbb{E}[X]) \ge \mathbb{E}[f(X)]\) ，得到对数证据的下界（ELBO）：

$$
\geq \arg\max_{c} \int P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c) \times \ln \frac {P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c)} {P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)} \, ds_{1:T}
$$

$$
= \arg\max_{c} \int P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c) \times (\ln {P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c)} - \ln {P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)}) \, ds_{1:T}
$$
由熵的定义式 \(H(\cdot) = - \int P(\cdot) \ln P(\cdot)\) ，有：

$$
=  \arg\max_{c} \{
\int {P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)} \ln P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c) \, ds_{1:T} + H[{P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c)}]
\}
$$

由于熵 \(H(\cdot) \ge 0\) ，进一步得到更简洁的可优化目标：

$$
\ge \arg\max_c \int P(s_{1:T} \mid o_{1:T,j}, a_{1:T,j}; c) \ln P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c) \, ds_{1:T} \tag{6}
$$

接下来用马尔可夫性质分解联合概率：

$$
P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c)
= P(s_1) \prod_{t=1}^T \Omega_j(o_{t,j} \mid s_t)
\prod_{t=1}^T \pi(a_{t,j} \mid o_{t,j}, s_t; c)
\prod_{t=1}^{T-1} T(s_{t+1} \mid s_t, a_t)
$$

因此

$$
\arg\max_c \int P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c) [\ln P(s_1) + \sum_{t=1}^T \ln \Omega_j(o_{t,j} \mid s_t) + \sum_{t=1}^T \ln \pi(a_{t,j} \mid o_{t,j}, s_t; c) + \int \sum_{t=1}^{T} \ln T(s_{t+1} \mid s_{t,j}, a_{t,-j})da_{1:T,-j}] ds_{1:T}
$$

式中转移概率由于不知道别的智能体的动作所以只能使用期望替代。

由于 \(\ln P(s_1)\) 、 \(\ln \Omega_j(\cdot)\) 、 \(\ln T(\cdot)\) 不依赖于角色 \(c\) ，对 \(c\) 的最大化时可忽略这些项，仅保留与策略相关的项：

$$
= \arg\max_c \int P(s_{1:T}, o_{1:T,j}, a_{1:T,j} \mid c) \sum_{t=1}^T \ln \pi(a_{t,j} \mid o_{t,j}, s_t; c) \, ds_{1:T} 
$$

策略在实现上只以观测 \(o_{t,j}\) 作为输入（MA-POMDP 中的 actor 不显式用 \(s_t\) ），因此可以忽略对于状态的积分：

$$
= \arg\max_c \sum_{t=1}^T \ln \pi(a_{t,j} \mid o_{t,j}; c) \tag{9}
$$

在混合动作空间下，策略对连续与离散分量进行因子分解： \(\pi(a_{t,j} \mid o_{t,j}; c) = \pi(a^c_{t,j} \mid o_{t,j}; c) \cdot \pi(a^d_{t,j} \mid o_{t,j}; c)\) ，于是

$$
= \arg\max_c \sum_{t=1}^T \Big[
\ln \pi(a^c_{t,j} \mid o_{t,j}; c) + \ln \pi(a^d_{t,j} \mid o_{t,j}; c)
\Big] \tag{10}
$$

其中离散分量的观测 \(a^d_{t,j}\) 已由环境产生；训练推断时仅需对策略离散分支在该值上的对数概率求和。

实现上，在每个观测-动作轨迹上，对目标智能体角色的计算式 \(\hat{c}_j\) ，采用梯度上升法优化 \(c_j\) 到最可能的角色，实现某个智能体对目标智能体角色的估计。

## 基于情景未来思维机制的预见性动作选择

### 动作预测

在"动作预测"这一步，具备多角色策略的智能体使用"预先推断的角色"与观测，来预测邻近智能体的动作。对目标智能体 \(j\) （ \(j \in E,\, j \ne i\) ），基于已训练的多角色策略 \(\pi_\phi\) 与已推断角色 \(\hat c_j\) 进行预测：

$$
\hat a_{t,j} \sim \pi_\phi(\cdot \mid o_{t,j}; \hat c_j).
$$

由此，其他智能体的预测动作集合 \(\hat a_{t,-i}\) 为：

$$
\hat a_{t,-i} = \langle \pi_\phi(o_{t,1}; \hat c_1),\, \cdots,\, \pi_\phi(o_{t,i-1}; \hat c_{i-1}),\, \pi_\phi(o_{t,i+1}; \hat c_{i+1}),\, \cdots,\, \pi_\phi(o_{t,N}; \hat c_N) \rangle.
$$

### 下一观测模拟

在"下一观测模拟"这一步，我们介绍如何用预测的动作集合 \(\hat a_{t,-i}\) 来模拟智能体的下一观测。注意：该预测来源于智能体 \(i\) 的"脑模拟"，其假设自身在当前时刻未执行动作，即 \(a_{t,i} = \varnothing\) （ \(\varnothing\) 表示"空动作"，即不采取任何动作）。这样做是为了模拟"其他目标智能体均已执行动作、但智能体 \(i\) 尚未执行"的时刻的观测。

定义下一观测模拟函数 \(D(\cdot)\) ：

$$
\hat o_{t+1,i} = D(o_{t,i}, \hat a_{t,-i},\, a_{t,i}=\varnothing).
$$

通过 \(\hat a_{t,-i}\) 与当前观测 \(o_{t,i}\) ，即可确定模拟的下一观测 \(\hat o_{t+1,i}\) 。使用 \(\hat o_{t+1,i}\) 进行动作选择，能够让智能体"忽略其他体动作的影响"，因为在脑模拟中，下一状态仅由自身动作 \(a_{t,i}\) 决定——其他体动作的影响已在 \(\hat o_{t+1,i}\) 中提前应用。

### 动作选择

当智能体得到模拟的下一观测 \(\hat o_{t+1,i}\) 后，即可进行前瞻性决策。智能体以 \(\hat o_{t+1,i}\) 与自身角色 \(c_i\) 作为输入，使用多角色策略 \(\pi_\phi\) 获得原型动作：

$$
a_{t,i}^\text{proto} = \{a^c_{t,i},\, \bar a^d_{t,i}\} = \pi_\phi(\cdot \mid \hat o_{t+1,i}; c_i),
$$

随后应用后处理器 \(g(\cdot)\) 得到执行的离散动作：

$$
a^d_{t,i} \leftarrow g(\bar a^d_{t,i}, W),\quad a_{t,i} = \{a^c_{t,i},\, a^d_{t,i}\}.
$$

换言之，智能体能够在"考虑其他体即将采取的行为"的前提下，选择更具适应性的当前动作。

## 半个结论

EFT 应该是一个在仅 act 过程中的模块，但是进一步的情况得仔细读代码了，这篇比 AgA 更不好读，公式推导已经给我干废一半了，代码阅读之后再说，粗看他使用的flow 和 rllib 框架，我没用过阅读也有点难度。

## 10.17改

代码这块，我不是很懂他的结构，应该是把各个部分都定义好，然后送到 flow 里训练，很多组件都在 flow 中定义好了。总体来看，训练过程中没有使用到 EFT 模块，只在 act 过程中使用了 EFT 模块。多角色策略的使用和定义在 characterinference 中，里面有轨迹收集和 loss 等，但我确实没有显式地看到训练过程。在 visualize 中只有加载，没有训练过程，训练过程在 train_rllib 中，但没有显式的训练结构。他提到的角色向量是指奖励中各部分的权重所构成的权重向量，使用训练好的多角色策略结合当前观测和权重向量即可获知下一个 action。他的状态转移非常简单，所以可以在不动用环境的前提下直接算出来，然后智能体根据下个状态做行动。思路就是这么个思路，细节一些的就有点难找了，譬如哪些信息是全局的、哪些又是局部的等等，只能借鉴思路了，迁移应该不大简单。

实验就不看了我觉得既然不方便迁移我就觉得没必要。

